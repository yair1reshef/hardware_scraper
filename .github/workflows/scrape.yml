name: Hardware Jobs Scraper

# Triggers:
# 1. repository_dispatch: API trigger from Google Sheets button
# 2. workflow_dispatch: Manual trigger from GitHub UI for testing
on:
  repository_dispatch:
    types: [trigger_scrape]
  workflow_dispatch:  # Enables manual runs from GitHub Actions tab

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      # Step 1: Checkout code
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      # Step 2: Set up Python
      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      # Step 3: Install Python dependencies
      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      # Step 4: Install Playwright browsers (CRUCIAL!)
      - name: Install Playwright Browsers
        run: |
          playwright install chromium
          playwright install-deps chromium
      
      # Step 5: Run the scraper
      - name: Run Hardware Jobs Scraper
        env:
          GOOGLE_CREDENTIALS_JSON: ${{ secrets.GOOGLE_CREDENTIALS_JSON }}
        run: |
          python main.py
      
      # Optional: Upload logs as artifacts (useful for debugging)
      - name: Upload Logs
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: scraper-logs
          path: |
            *.log
            *.txt
          retention-days: 7
